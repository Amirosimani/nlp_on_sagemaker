{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade to the latest `sagemaker` version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import TrainingJobAnalytics\n",
    "from sagemaker.huggingface import HuggingFace, HuggingFaceModel, HuggingFacePredictor\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permissions\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"huggingface_classifier\"\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Womens Clothing E-Commerce Reviews.csv')\n",
    "df = df[['Review Text',\t'Rating']]\n",
    "df.columns = ['text', 'label']\n",
    "df['label'] = df['label'] - 1\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "train, validate, test = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('./data/train.csv', index=False)\n",
    "validate.to_csv('./data/validate.csv', index=False)\n",
    "test.to_csv('./data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file('./data/train.csv', bucket,\n",
    "                      f'{prefix}/data/train.csv')\n",
    "s3_client.upload_file('./data/validate.csv', bucket,\n",
    "                      f'{prefix}/data/validate.csv')\n",
    "s3_client.upload_file('./data/test.csv', bucket,\n",
    "                      f'{prefix}/data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a HuggingFace Transformers fine-tuning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.getLevelName(\"INFO\"),\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger.info(sys.argv)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train-batch-size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval-batch-size\", type=int, default=64)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--model_name\", type=str)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "    parser.add_argument(\"--output_dir\", type=str)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.getLevelName(\"INFO\"),\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "   \n",
    "    # download tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    \n",
    "    # Load dataset\n",
    "    train_file = f\"{args.training_dir}/train.csv\"\n",
    "    validate_file = f\"{args.test_dir}/validate.csv\"\n",
    "    dataset = load_dataset('csv', data_files={'train': train_file,\n",
    "                                             'test': validate_file})\n",
    "    \n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "    logger.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "    logger.info(f\" loaded test_dataset length is: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "    # tokenizer helper function\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "    # tokenize dataset\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "    \n",
    "\n",
    "    # set format for pytorch\n",
    "    train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "    test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    logger.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "    logger.info(f\" loaded test_dataset length is: {len(test_dataset)}\")\n",
    "\n",
    "    # compute metrics function for binary classification\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "    # download model from model hub\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=5)\n",
    "\n",
    "    # define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        learning_rate=float(args.learning_rate),\n",
    "    )\n",
    "\n",
    "    # create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    if get_last_checkpoint(args.output_dir) is not None:\n",
    "        logger.info(\"***** continue training *****\")\n",
    "        trainer.train(resume_from_checkpoint=args.output_dir)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    # evaluate model\n",
    "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    # writes eval result to file which can be accessed later in s3 ouput\n",
    "    with open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "        print(f\"***** Eval results *****\")\n",
    "        for key, value in sorted(eval_result.items()):\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    # Saves the model to s3\n",
    "    trainer.save_model(args.model_dir)\n",
    "    tokenizer.save_pretrained(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an HuggingFace Estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased',\n",
    "                 'output_dir':'/opt/ml/checkpoints'\n",
    "                 }\n",
    "\n",
    "\n",
    "\n",
    "metric_definitions=[\n",
    "    {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'learning_rate', 'Regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy', 'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1', 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_precision', 'Regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_recall', 'Regex': \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_runtime', 'Regex': \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_samples_per_second', 'Regex': \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 uri where our checkpoints will be uploaded during training\n",
    "job_name = \"using-spot\"\n",
    "checkpoint_s3_uri = f's3://{bucket}/{job_name}/checkpoints'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./src',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            size=5,\n",
    "                            instance_count=1,\n",
    "                            base_job_name=job_name,\n",
    "                            checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "#                             use_spot_instances=True,\n",
    "#                             max_wait=3600, # This should be equal to or greater than max_run in seconds'\n",
    "#                             max_run=1000, # expected max run in seconds\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters,\n",
    "                            metric_definitions=metric_definitions\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excute the fine-tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': f\"s3://{bucket}/{prefix}/data/train.csv\",\n",
    "        'test': f\"s3://{bucket}/{prefix}/data/validate.csv\"\n",
    "       }\n",
    "\n",
    "huggingface_estimator.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captured metrics can be accessed as a Pandas dataframe\n",
    "df = TrainingJobAnalytics(training_job_name=huggingface_estimator.latest_training_job.name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'huggingface-finetune'\n",
    "\n",
    "predictor = huggingface_estimator.deploy(initial_instance_count=1,\n",
    "                                         instance_type=\"ml.m5.xlarge\",\n",
    "                                         endpoint_name = model_name\n",
    "#                                          accelerator_type='ml.eia2.medium'  # for EI\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload = {\"inputs\": test['text'].tolist()}\n",
    "# predictor.predict(payload)\n",
    "# TODO: Neo compilation, elastic endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for idx, row in test.iterrows():\n",
    "    payload = {\"inputs\": row['text']}\n",
    "    pred = predictor.predict(payload)[0]\n",
    "    \n",
    "    # rename label to prediction\n",
    "    pred['prediction'] = pred.pop('label')\n",
    "    # convert prediction value to int\n",
    "    pred['prediction'] = int(pred['prediction'].replace('LABEL_', ''))\n",
    "    pred_list.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['prediction'] = pred_list\n",
    "df_test = pd.concat([test.drop(['prediction'], axis=1), test['prediction'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df_test['label'], df_test['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying the endpoint - HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'huggingface-finetune'\n",
    "\n",
    "# # create Hugging Face Model Class\n",
    "# huggingface_model = HuggingFaceModel(\n",
    "#    model_data=huggingface_estimator.model_data, # S3 path to your trained sagemaker model\n",
    "#    role=role, # IAM role with permissions to create an Endpoint\n",
    "#     transformers_version='4.6',\n",
    "#     tensorflow_version='2.4',\n",
    "#     py_version='py37',\n",
    "#     name=model_name\n",
    "# )\n",
    "\n",
    "# # deploy model to SageMaker Inference\n",
    "# predictor = huggingface_model.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type=\"ml.m5.xlarge\",\n",
    "# #     accelerator_type='ml.eia2.medium'  # for EI\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invoke iwth the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = boto3.client('sagemaker')\n",
    "# endpoint = client.list_endpoints()['Endpoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"inputs\": [\"I love using the new Inference DLC.\"]}\n",
    "\n",
    "predictor = HuggingFacePredictor(endpoint_name=endpoint_name,\n",
    "                                sagemaker_session=sess\n",
    "                                )\n",
    "predictor.predict(data=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: invoke with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "payload = {\"inputs\":[\"I love using the new Inference DLC.\"]}\n",
    "user_encode_data = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                          Body=user_encode_data,\n",
    "                          ContentType='application/json'\n",
    "                         )\n",
    "response['Body'].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CITATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
